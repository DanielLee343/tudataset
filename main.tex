\documentclass{article}

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{booktabs}
\usepackage{url}
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{tikz}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{thmtools}		
\usepackage{mleftright}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{remark}[theorem]{Remark}

\usepackage{thm-restate}
\usepackage[mathic=true]{mathtools}
\usepackage{fixmath}
\usepackage{siunitx}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0.5\topsep}
\setlist[description]{noitemsep, topsep=0.5\topsep}
\setlist[itemize]{noitemsep, topsep=0.5\topsep}

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0.5\topsep}
\setlist[description]{noitemsep, topsep=0.5\topsep}
\setlist[itemize]{noitemsep, topsep=0.5\topsep}

\usepackage{setspace}
\usepackage[protrusion=true,expansion=false, activate={true,nocompatibility},final,kerning=true,spacing=true]{microtype}

\usepackage{ellipsis}
\usepackage{xspace}
\usepackage{hfoldsty}

\usepackage{ifthen}
\newcommand{\CC}[1][]{$\text{C\hspace{-.25ex}}^{_{_{_{++}}}}
	\ifthenelse{\equal{#1}{}}{}{\text{\hspace{-.625ex}#1}}$}

\newcommand{\win}[1]{$\hspace{-0.3mm}$\textbf{#1}}
\newcommand{\sd}[1]{\scriptsize{$\pm$#1}}


% Let cleveref and thmtools work together
\makeatletter
\def\thmt@refnamewithcomma #1#2#3,#4,#5\@nil{%
	\@xa\def\csname\thmt@envname #1utorefname\endcsname{#3}%
	\ifcsname #2refname\endcsname
	\csname #2refname\expandafter\endcsname\expandafter{\thmt@envname}{#3}{#4}%
	\fi
}
\makeatother
\usepackage[capitalise,noabbrev]{cleveref}   

\newcommand{\new}[1]{\emph{#1}}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\newcommand{\cA}{\ensuremath{{\mathcal A}}\xspace}
\newcommand{\cB}{\ensuremath{{\mathcal B}}\xspace}
\newcommand{\cC}{\ensuremath{{\mathcal C}}\xspace}
\newcommand{\cD}{\ensuremath{{\mathcal D}}\xspace}
\newcommand{\cF}{\ensuremath{{\mathcal F}}\xspace}
\newcommand{\cH}{\ensuremath{{\mathcal H}}\xspace}
\newcommand{\cN}{\ensuremath{{\mathcal N}}\xspace}
\newcommand{\cO}{\ensuremath{{\mathcal O}}\xspace}
\newcommand{\cP}{\ensuremath{{\mathcal P}}\xspace}
\newcommand{\cR}{\ensuremath{{\mathcal R}}\xspace}
\newcommand{\cS}{\ensuremath{{\mathcal S}}\xspace}
\newcommand{\cU}{\ensuremath{{\mathcal U}}\xspace}
\newcommand{\cV}{\ensuremath{{\mathcal V}}\xspace}
\newcommand{\cPn}{\ensuremath{{\mathcal P}_n}\xspace}

\newcommand{\fA}{\ensuremath{\mathfrak{A}}\xspace}
\newcommand{\fB}{\ensuremath{\mathfrak{B}}\xspace}
\newcommand{\fC}{\ensuremath{\mathfrak{C}}\xspace}

\newcommand{\fa}{\ensuremath{\mathfrak{a}}\xspace}
\newcommand{\fb}{\ensuremath{\mathfrak{b}}\xspace}
\newcommand{\fc}{\ensuremath{\mathfrak{c}}\xspace}
\newcommand{\fd}{\ensuremath{\mathfrak{d}}\xspace}

\newcommand{\bA}{\ensuremath{{\bf A}}\xspace}
\newcommand{\bB}{\ensuremath{{\bf B}}\xspace}
\newcommand{\bK}{\ensuremath{{\bf K}}\xspace}
\newcommand{\bE}{\ensuremath{{\bf E}}\xspace}
\newcommand{\bN}{\ensuremath{{\bf N}}\xspace}
\newcommand{\bG}{\ensuremath{{\bf G}}\xspace}

\newcommand{\ba}{\ensuremath{{\bf a}}\xspace}
\newcommand{\bb}{\ensuremath{{\bf b}}\xspace}
\newcommand{\bc}{\ensuremath{{\bf c}}\xspace}

\newcommand{\bbE}{\ensuremath{\mathbb{E}}}
\newcommand{\bbR}{\ensuremath{\mathbb{R}}}
\newcommand{\bbQ}{\ensuremath{\mathbb{Q}}}
\newcommand{\bbP}{\ensuremath{\mathbb{P}}}
\newcommand{\bbZ}{\ensuremath{\mathbb{Z}}}
\newcommand{\bbN}{\ensuremath{\mathbb{N}}}
\newcommand{\bbNn}{\ensuremath{\mathbb{N}_0}}

\newcommand{\bbRnp}{\ensuremath{\bbR_{\geq 0}}}
\newcommand{\bbQnp}{\ensuremath{\bbQ_{\geq}}}
\newcommand{\bbZnp}{\ensuremath{\bbZ_{\geq}}}

\newcommand{\bbRsp}{\ensuremath{\bbR_>}}
\newcommand{\bbQsp}{\ensuremath{\bbQ_>}}
\newcommand{\bbZsp}{\ensuremath{\bbZ_>}}

\newcommand{\cp}{\textsf{P}\xspace}
\newcommand{\cnp}{\textsf{NP}\xspace}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\GN}{\mathbb{G}_n}
\newcommand{\rb}{\right\}\xspace}
\newcommand{\lb}{\left\{\xspace}
\newcommand{\lbr}{\left(\xspace} 
\newcommand{\rbr}{\right)\xspace}
\newcommand{\ndelta}{\ensuremath{\overline{\delta}}}
\newcommand{\oms}{\{\!\!\{}
\newcommand{\cms}{\}\!\!\}}
\newcommand{\trans}{^T}
\renewcommand{\vec}[1]{\mathbf{#1}}

\newcommand{\nk}[1]{{{\textcolor{brown}{\textbf{[NK:} {#1}\textbf{]}}}}}
\newcommand{\cm}[1]{{{\textcolor{red}{\textbf{[CM:} {#1}\textbf{]}}}}}

\usepackage{ifthen}
\newcommand{\CPP}[1][]{$\text{C\hspace{-.25ex}}^{_{_{_{++}}}}
	\ifthenelse{\equal{#1}{}}{}{\text{\hspace{-.625ex}#1}}$}


%\renewcommand*{\familydefault}{ttdefault}
\usepackage{listings}
\usepackage{setspace}
\definecolor{Code}{rgb}{0,0,0}
\definecolor{Decorators}{rgb}{0.5,0.5,0.5}
\definecolor{Numbers}{rgb}{0.5,0,0}
\definecolor{MatchingBrackets}{rgb}{0.25,0.5,0.5}
\definecolor{Keywords}{rgb}{0,0,1}
\definecolor{self}{rgb}{0,0,0}
\definecolor{Strings}{rgb}{0,0.63,0}
\definecolor{Comments}{rgb}{0,0.63,1}
\definecolor{Backquotes}{rgb}{0,0,0}
\definecolor{Classname}{rgb}{0,0,0}
\definecolor{FunctionName}{rgb}{0,0,0}
\definecolor{Operators}{rgb}{0,0,0}
\definecolor{Background}{rgb}{0.98,0.98,0.98}
\lstdefinelanguage{Python}{
	numbers=left,
	numberstyle=\footnotesize,
	numbersep=1em,
	xleftmargin=1em,
	framextopmargin=2em,
	framexbottommargin=2em,
	showspaces=false,
	showtabs=false,
	showstringspaces=false,
	frame=l,
	tabsize=4,
	% Basic
	basicstyle=\ttfamily\small\setstretch{1},
	backgroundcolor=\color{Background},
	% Comments
	commentstyle=\color{Comments}\slshape,
	% Strings
	stringstyle=\color{Strings},
	morecomment=[s][\color{Strings}]{"""}{"""},
	morecomment=[s][\color{Strings}]{'''}{'''},
	% keywords
	morekeywords={import,from,class,def,for,while,if,is,in,elif,else,not,and,or,print,break,continue,return,True,False,None,access,as,,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert},
	keywordstyle={\color{Keywords}\bfseries},
	% additional keywords
	morekeywords={[2]@invariant,pylab,numpy,np,scipy},
	keywordstyle={[2]\color{Decorators}\slshape},
	emph={self},
	emphstyle={\color{self}\slshape},
	%
}
\usepackage{icml2020}





\icmltitlerunning{TUD Datasets}

\begin{document}

\twocolumn[
\icmltitle{TUDataset: A collection of benchmark datasets for learning with graphs}
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{C. Morris}{p}
\icmlauthor{N.\,M.\,Kriege}{vie}
\icmlauthor{Franka Bause}{vie}
\icmlauthor{K. Kersting}{darm}
\icmlauthor{P. Mutzw}{bonn}
\icmlauthor{N. Neumann}{wustl}
\end{icmlauthorlist}

\icmlaffiliation{p}{CERC in Data Science for Real-Time Decision-Making, Polytechnique Montr√©al}
\icmlaffiliation{vie}{University of Vienna}
\icmlaffiliation{darm}{Machine Learning Group, TU Darmstadt}
\icmlaffiliation{bonn}{Department of Computer Science, University of Bonn}
\icmlaffiliation{wustl}{Washington University in St. Louis}
\icmlcorrespondingauthor{Christopher Morris}{christopher.morris@tu-dortmund.de}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recently, there has been an increasing interest in (supervised) learning with graph data, especially using graph neural networks.  However, the development of meaningful benchmark datasets and standardized evaluation procedures is lagging. That is, most papers still evaluate their methods on small-scale datasets leading to high standard deviations and hard to interpret results, consequently hindering advancements in this area. To address this, we introduce the \textsc{TuDataset} for graph classification and regression. The dataset consists of over 120 datasets from a wide range of applications and varying sizes. We provide Python-based data loaders, kernel and graph neural network baseline methods implementations, and evaluation tools. Here, we give an overview of the datasets, evaluation tools, and provide baseline experiments. All datasets can be obtained from \url{www.graphlearning.io}. The experiments are fully reproduceable from the code available at \url{www.github.com/chrsmrrs/tudatasets}.
\end{abstract}

\section{Introduction}
Graph-structured data is ubiquitous across application domains ranging from chemo- and bioinformatics to image and social network analysis. To develop successful machine learning models in these domains, we need techniques that can exploit the rich information inherent in the graph structure, as well as the feature information contained within nodes and edges. In recent years, numerous approaches have been proposed for machine learning with graphs---most notably, approaches based on graph \new{kernels}~\cite{Kri+2019} or using \new{graph neural networks} (GNNs)~\cite{Gil+2017}. However, most papers, even recent ones, evaluate newly proposed architectures or methods on a fixed set of small-scale benchmark datasets leading to high standard deviations and hard to interpret results. 

Here, we give an overview of \textsc{Tud datasets}. The benchmark collection consists of over 120 datasets from a wide range of domains for supervised learning with graphs, i.e., classificiation and regression. All datasets are provided in a common dataset format at \url{graphlearning.io}, and can easily be accessed from popular graph learning frameworks such as \emph{Pytorch Geometric}~\cite{Fey+2019} and \emph{DGL}~\cite{Wan+2019}.

\paragraph{Related work.}
There exists two main approaches to supervised learning with graphs, graph kernels and graph neural networks (GNNs). Graph kernels have been studied extensively in the past 15 years, see ~\cite{Kri+2019} for a thorough overview. 
Important approaches include random-walk and shortest paths based kernels~\cite{Gaertner2003,Sugiyama2015,Bor+2005,Kri+2017b}, as well as the Weisfeiler-Lehman subtree kernel~\cite{She+2011,Mor+2017}. 
Further recent works focus on assignment-based approaches~\cite{Kri+2016,Nik+2017}, spectral approaches~\cite{Kon+2016}, and graph decomposition approaches~\cite{Nik+2018}.

\cm{Add new work from Borgwardt and Pascale Frossard.}

Recently, GNNs~\cite{Gil+2017} emerged as a alternative to graph kernels. Notable instances of this model include~\cite{Duv+2015},~\cite{Li+2016},~\cite{Ham+2017} and the spectral approaches proposed in~\cite{Bru+2014,Def+2015,Kip+2017}---all of which descend from early work in~\cite{Kir+1995,Mer+2005,Sca+2009}. More expressive higher-order approaches are proposed in~\cite{Mor+2019,Mar+2019}. A unifying message passing architecture can be found in~\cite{Gil+2017}. Two recent surveys~\cite{wu2019comprehensive,zhou2018graph} provide a thorough overview of graph neural networks.

The papers~\cite{Fey+2019,Err+2019,Dwi+2020} evalute GNNs using a unified evaluation procedure, however, they only use small- or medium scale datasets. Recently, \url{ogb.stanford.edu} launchend, however the provided datasets for graph classification focus on chemistry and bioinformatic applications, and the number is quite limited at this point.

\paragraph{Contributions} We give an overview of \textsc{TuDataset}, its unified evalution procedures, and baseline methods. Moreover, we report results on a experimental study comparing graph kernels and GNNs on a subset of the \textsc{TuDataset}.

\section{The TUDataset collection}

The \textsc{TuDataset} contains over 120 datasets provided at \url{graphlearning.io}. The data  baseline methods and experimental evaluation tools can be installed by running \texttt{pip install tudatasets}. See the website for dataset statistics and further documentation.

\subsection{Datasets}

Our collection of datasets covers graphs from various domains, which were provided by different authors. Therefore, they differ regarding the used graph model even within the same domain and the provided annotations, e.g., discrete or continuous node and edge attributes. Here, we give an overview of some representative domains and graph models.

\paragraph{Small molecules.} 
A commonly used class of graph datasets are derived from small molecules with class labels representing, e.g., toxicity or biological activity determined in the context of drug discovery. A molecule is represented by a graph in which nodes take the places of atoms and edges that of chemical bonds. Consequently, the labels indicate atom and bond types, where additional chemical attributes are possible. The graph models differs, e.g., in whether hydrogen atoms represented explicitly by nodes and bonds in aromatic rings are annotated accordingly.
Our collection contains small datasets commonly used in the early graph kernel literature such as \textsc{Mutag}~\citep{Deb+1991} and \textsc{Ptc}~\cite{Helma2001}, medium-sized datasets, e.g., \textsc{Nci1} and \textsc{Nci109}~\citep{Wal+2008,She+2011}, as well as several large datasets derived from the \emph{Tox21 Challenge 2014} or \emph{PubChem}~\citep{Kim2018}. This includes the 11 datasets from anticancer screen tests with different cancer cell lines used by \citet{Yan2008} to demonstrate the efficacy of classifiers based on significant graph patterns. These datasets, the largest of which contain more than 79k graphs, are typically not balanced and contain far more small molecules that are identified as inactive against cancer cells.
The \textsc{Alchemy} was proposed by~\citet{Chen2019a} for benchmarking {AI} models.

\cm{Mention \textsc{ZINC} and the also mention size of DS, 200k}
\cm{Mention \textsc{QM9}k}

For some cheminformatics prediction tasks, the geometrical arrangement of atoms has been determined and should be taken into account in a rotation-invariant manner. In order to achieve this, complete graphs with edges annotated by distances are commonly used~\citep{Kri+2012,Gil+2017}. Our collection contains the four datasets \textsc{Bzr}, \textsc{Cox2}, \textsc{Dhfr}, \textsc{Er} initially published by \citet{Sut+2003} in two versions, with 3d coordinates of atoms and inter-atom distances, respectively, cf.~\citep{Mah+2006,Kri+2012}.

\paragraph{Bioinformatics.}
The datasets \textsc{DD}, \textsc{Enzymes} and \textsc{Proteins} represent macromolecules. \citet{Bor+2005a} introduced a graph model for proteins, where nodes represent secondary structure elements and are annotated by their type, i.e., helix, sheet or turn, as well as several physical and chemical information. Two nodes are connected by an edge if they are neighbors along the amino acid sequence or one of three nearest neighbors in space. Using this approach, the dataset \textsc{Enzymes} was derived from the BRENDA database~\citep{Schomburg2004}. Here, the task is to assign enzymes to one of the 6 EC top level classes, which reflect the chemical reaction they catalyze. Similarly, the dataset \textsc{Proteins} was derived from \citep{Dob+2003} and comes with the task to predict whether a protein is an enzyme. The dataset \textsc{DD} used by \citet{She+2011} is based on the same data, but contains graphs, where nodes represent individual amino acids and edges their spatial proximity.


\paragraph{Computer vision.}
Graph-based methods are widely used in computer vision for various tasks using diverse graph models. Our collection provides several datasets originating from the \emph{IAM Graph Database}~\citep{Riesen2008} such as \textsc{Letter} and \textsc{Fingerprint}. Other datasets represent \textsc{Cuneiform} signs~\citep{Kriege2018}, 3d point clouds for robot grasping tasks (\textsc{FirstMM\_DB}) and semantic image processing (\textsc{Msrc})~\citep{Neu+2016}.

\paragraph{Social networks.}
\citet{Yan+2015a} introduced several graph classification datasets derived from social networks. In the \textsc{Reddit} datasets each graph represents a discussion thread, where nodes correspond to users, two of which are connected by an edge, if one responded to a comment of the other. This graph model is used to derive several datasets, where the classification task is to distinguish either discussion-based and question/answer-based subreddits (\textsc{Reddit-Binary}) or predict the subreddit, where the thread was posted (\textsc{Reddit-Multi-5K} and \textsc{Reddit-Multi-12K}). \textsc{Collab} is dataset derived from scientific collaboration networks. Each graph is the ego-network of a researcher and the task is to predict his research field, i.e., High Energy, Condensed Matter or Astro Physics. Similarly, the \textsc{Imdb} datasets consist of ego-networks derived from actor collaborations and the task is to predict the genre, e.g.,  Action vs.\@ Romance.
Similar approaches were used by~\citet{Rozemberczki2020} to obtain larger social network datasets. \textsc{Reddit\_Threads} contains more than 200k graphs with the task to predict whether a thread is discussion based. \textsc{Deezer\_Ego\_Nets} and \textsc{Twitch\_Egos} contain ego-networks derived from online services and the task is to predict the gender and play beahavior (single or multiple games) of the central user.
\textsc{GitHub\_Stargazers} contains graphs representing the social networks of GitHub users divided into those who starred popular machine learning and web development repositories.


Recently, temporal graphs were considered by \citet{Oettershagen2019}, where edges represent the contact or interaction between two individuals at a certain point in time. These graphs are of interest when studying dissemination processes such as the spreading of epidemics, rumours or fake news. We provide temporal graph classification datasets derived from \textsc{Tumblr}~\citep{rozenshtein2016reconstructing}, \textsc{Dblp} and \textsc{Facebook}~\citep{viswanath2009evolution} as well as contacts between students at the \textsc{MIT}~\cite{konect:eagle06}, in a \textsc{Highschool} and visitors at the \textsc{Infectious} exhibition~\citep{Isella2011}.

\paragraph{Synthetic.}
Several graph datasets contain synthetic instances that where typically generated to demonstrate the strengths or weaknesses of specific methods. The datasets \textsc{SyntheticNew} and \textsc{SyntheticNew} were created by \citet{Fer+2013} (see Erratum) and \citet{Mor+2016}, respectively, to demonstrate the ability of kernels to operate on graphs with continuous attributes.
With the same goal, the dataset \textsc{Frankenstein} was obtained from a small molecule dataset by replacing the atom labels by MNIST digit images~\citep{Ors+2015}

\citet{Knyazev2019} introduced the datasets \textsc{Colors} and \textsc{Triangles}, where the task is to count the number of nodes with a given one-hot-encoded color and the number of triangles, respectively. These classification tasks were considered particularly challenging for graph neural networks.

\subsection{Baselines Methods}

To provide meaningful baselines, we provide implementations of common graph kernel as well as GNNs baselines. We have implemented the \new{Weisfeiler-Lehman Subtree}~\cite{She+2011}, \new{Shortest-path}~\cite{Bor+2005}, \new{Graphlet}~\cite{She+2009} \nk{Das ist eine Variante mit Labeln und 3 Knoten oder? CM: Ja}, \new{Weisfeiler-Lehman Optimal Assigment}~\cite{Kri+2016} kernel, as well as the higher-order WL kernels~\cite{Mor+2019b}, in \CPP and made them accessible through the Python interface of \textsc{Tud dataset}, see the website for further details.  Moreover, all GNN architectures provided by PyTorch Geometric can be conveniently used as a baseline as well. 

\subsection{Evaluation Methods}\label{eval}

To ensure a fair and meaningful comparison between methods, we propose the following evaluation procedures for kernels and GNN approaches. All proposed methods can be conveniently accessed from the Python interface, see the website for further details. First, for kernels we propose the established $C$-SVM implementation \textsc{LibSvm}~\cite{Cha+11} for kernels that compute a Gram matrix, and the linear $C$-SVM implementation \textsc{LibLinear}~\cite{Fan+2008} for kernels  that can be computed based on sparse, explicit feature maps. We optimize GNNs end-to-end using \textsc{Adam}~\cite{}. Too compute accuracies and MAE, we propose to use $10$-fold cross-validation, where we select a validation set uniformly at random from each training fold ($10\%$ of the training fold), to select hyperparameters, e.g., number of iterations, $C$ parameter, number of layers, feature dimension, dots Moreover, we repeat the above evaluation ten times and report standard deviations over all ten repetitions, and additionally across all one hundred runs (i.e., ten repetitions with ten folds each.). See the appendix for examples balbalb

\section{Experimental evaluation}

\cm{work on this}

Our intention here is to provide baseline experiments and compare graph kernels and GNNs. More precisely, we address the following questions:
\begin{description}
	\item[Q1] Are GNNs superior to graph kernels? Is there a single method that dominates?
	\item[Q2] How do kernels compare to GNNs on large-scale molecular learning tasks.
\end{description}

\subsection{Experimental protocol}

We used the following datasets, graph kernels, and GNN baselines.

\paragraph{Datasets.} We used the \textsc{deezer\_ego\_nets}, \textsc{github\_stargazers}, \textsc{Enymes},
\textsc{Imdb-BINARY}, \textsc{Imdb-MULTI}, \textsc{Mcf-7}, \textsc{Molt-4},	\textsc{Nci1}, \textsc{Proteins},
\textsc{Reddit-Binary},	\textsc{reddit\_threads}, \textsc{twitch\_egos}, \textsc{Uacc257}. See the website for dataset statistics.
\cm{ZINC ALche}

\paragraph{Graph kernels.} As kernel baselines we used the \new{Weisfeiler-Lehman Subtree}~\cite{She+2011}, \new{Shortest-path}~\cite{Bor+2005}, \new{Graphket}~\cite{She+2009}, \new{Weisfeiler-Lehman Optimal Assigment} \textsc{WL-OA}~\cite{Kri+2016}, and $\delta$-$2$-LWL$^+$ kernel~\cite{Mor+2019b} included in the \textsc{TUD Dataset} package. The $C$-parameter was selected from $\{10^{-3}, 10^{-2}, \dotsc, 10^{2},$ $10^{3}\}$ from the validation set. For the larger datasets, we computed sparse feature vectors for each graph and used the linear $C$-SVM implementation of \text{Liblinear}~\cite{Fan+2008}. The number of iterations of the \textsc{$1$-WL}, \textsc{WL-OA}, and the  $\delta$-$2$-LWL$^+$ were selected from $\{0,\dotsc,5\}$.\footnote{As already shown in~\cite{She+2011}, choosing the number of iterations too large will lead to overfitting.}\cm{Cite sklearn}

\paragraph{GNNs.} We used the GNN architectures \textsc{GCN}~\cite{Kip+2017}, \textsc{Sage}~\cite{Ham+2017}, \textsc{Gin} and \textsc{Gin-$\varepsilon$}~\cite{Xu+2018b} as neural baselines. For all experiments, we used the global mean operator to obtain graph-level outputs. For each dataset, we optimize the number of hidden units from $
\{16, 32, 64, 128\}$, the number of layers from $ \{2, 3, 4, 5\}$ and the learning rate from $ \{0.0001, 0.001, 0.01 \}$ with respect to the validation set. The batch size was fixed to $128$.\cm{change this }

For both methods, we used the evaluation procecude described in~\cref{eval} to optimize hyperparameters and compute accuracies. All experiments can be reproduced using the scripts provided at \url{XXX}.

\cm{Add regression}

\subsection{Results and discussion}
See~\cref{t2l}.





\begin{table*}[t]\centering		
	\caption{Classification accuracies in percent and standard deviations,  \textsc{OOT}--- Computation did not finish within one day, \textsc{OOM}--- Out of memory.}
	\label{t2l}	
	\resizebox{0.8\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.1}
		\begin{tabular}{@{}c <{\enspace}@{}lcccccc@{}}	\toprule
			
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{6}{c}{\textbf{Dataset}}\\\cmidrule{3-8}
			& & {\textsc{Enyzmes}}         &  {\textsc{Imdb-Binary}}      & {\textsc{Imdb-Multi}}           & {\textsc{NCI1}}       & {\textsc{Proteins}}           & {\textsc{Reddit-Binary}}     \\	\toprule
			\multirow{4}{*}{\rotatebox{90}{Kernel}}	& \text{1-WL}  &    \scriptsize $\pm  \pm  $       & \scriptsize $\pm   \pm $ &  \scriptsize $\pm  \pm   $ &  \scriptsize $\pm  \pm $ &  \scriptsize $\pm  \pm   $ & \scriptsize $\pm  \pm $  \\
				& \text{2-LWL} &   \scriptsize $\pm \pm $       &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $ & \scriptsize $\pm   \pm $ &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $   \\
				& \text{2-LWL$^+$} &   \scriptsize $\pm \pm $       &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $ & \scriptsize $\pm   \pm $ &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $   \\
				& \text{WLOA} &   \scriptsize $\pm \pm $       &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $ & \scriptsize $\pm   \pm $ &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $   \\
				& \text{GR}            &     \scriptsize $\pm  \pm$       &  \scriptsize $\pm \pm $ &  \scriptsize $\pm \pm   $ &  \scriptsize $\pm \pm $ &  \scriptsize $\pm \pm $ & \scriptsize $\pm  \pm  $   \\
				& \text{SP}            &   \scriptsize $\pm  \pm $       &  \scriptsize $\pm  \pm  $ &  \scriptsize $\pm  \pm $ &  \scriptsize $\pm \pm  $  & \scriptsize $\pm  \pm $ &  \scriptsize $\pm\pm $  \\
			\cmidrule{2-8}	
			\multirow{2}{*}{\rotatebox{90}{GNN}}	& \text{GIN-$\varepsilon$}    & \scriptsize $\pm \pm $       &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $ & \scriptsize $\pm   \pm $ &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $   \\
		& \text{GIN-$\varepsilon$-JK} &          \scriptsize $\pm \pm $       &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $ & \scriptsize $\pm   \pm $ &  \scriptsize $\pm \pm $ &  \scriptsize $\pm  \pm  $   \\
			\bottomrule
	\end{tabular}}
\end{table*}		

Bitte eintragen
IMDB-BINARY GINWithJK 72.93333333333334 0.5185449728701322 4.122566622330749
IMDB-BINARY GIN 72.06666666666668 0.3091206165165233 4.024370206076419
IMDB-MULTI GINWithJK 50.644444444444446 1.01153837126587 4.59462917806765
IMDB-MULTI GIN 48.84444444444444 0.37051848890073463 3.523185988018918
NCI1 GINWithJK 79.67558799675588 0.4130683399269047 1.5714739782362184
NCI1 GIN 78.01297648012977 0.38333502994634977 1.9142541830162463
PROTEINS GINWithJK 71.27895752895752 0.9574183044143284 4.459876910576175
PROTEINS GIN 71.16527241527241 0.3211668436757545 3.612551907408368
REDDIT-BINARY GINWithJK 90.31666666666668 0.4326918328582383 1.9894862541760765
REDDIT-BINARY GIN 90.18333333333334 0.39228674319799567 2.169805418823438

ENZYMES WL1 50.28333333333334 1.1524129275375015 6.872065030089411
ENZYMES LWL2 55.6 1.5709162223930901 6.788552455748164
ENZYMES LWLP2 53.55 1.5402921800749356 6.3274314606235675
ENZYMES WLOA 56.7 2.0094222497468714 7.129671957546308
ENZYMES GR 32.35 1.036688102672266 5.359804100897718
ENZYMES SP 39.8 1.2840906856172143 6.473364229243128
IMDB-BINARY WL1 72.57 0.9143850392476894 4.133412633647891
IMDB-BINARY LWL2 73.37 0.6372597586541924 4.430925411243119
IMDB-BINARY LWLP2 76.65 1.353698637068088 4.924175057814253
IMDB-BINARY WLOA 72.69 0.6818357573492301 4.749094650562357
IMDB-BINARY GR 59.19 0.865390085452797 5.017359863513877
IMDB-BINARY SP 57.860000000000014 1.4291256067959874 5.734143353631822
IMDB-MULTI WL1 50.09333333333333 0.6262409901769259 3.811118237764636
IMDB-MULTI LWL2 50.15333333333333 0.38418745424597134 4.055016098063676

IMDB-MULTI LWLP2 74.01333333333334 0.9360911636516311 4.502818870194288
IMDB-MULTI WLOA 49.86666666666666 0.6352602266438174 4.455209185760966
IMDB-MULTI GR 39.17333333333333 0.7660577291852385 4.380887784202853
IMDB-MULTI SP 39.50666666666666 0.18184242262647857 3.3791780723194287
NCI1 WL1 84.38929440389293 0.25125704415427325 1.6049986739634532
NCI1 LWL2 84.89537712895375 0.20028336376892103 1.7608176856505284
NCI1 LWLP2 84.32846715328466 0.35931558467907326 1.9074851647350406
NCI1 WLOA 84.985401459854 0.26237917572915365 1.944431716559596
NCI1 GR 65.8272506082725 0.31011161731748405 2.3057620215984294
NCI1 SP 74.21411192214111 0.26335881513838716 2.007846272147655
PROTEINS WL1 73.49348455598457 0.6459030975700468 4.234672201391122
PROTEINS LWL2 74.79681467181469 0.665180097352863 4.60334576380455
PROTEINS LWLP2 75.63296332046333 0.5832232093036747 4.118575468095003
PROTEINS WLOA 73.48656692406692 0.6497356451647441 4.4435644964869745
PROTEINS GR 71.91473616473615 0.5582152728034838 4.153766329154282
PROTEINS SP 75.29930823680823 0.433922307671726 3.6869730552272064
REDDIT-BINARY WL1 73.15 0.5860887304837047 3.1547583108694712
REDDIT-BINARY LWL2 89.09 0.4856953777832335 2.504376169827528





\begin{table*}[t]\centering		
	\caption{Classification accuracies in percent and standard deviations,  \textsc{OOT}--- Computation did not finish within one day, \textsc{OOM}--- Out of memory.}
	\label{t2l}	
	\resizebox{0.75\textwidth}{!}{ 	\renewcommand{\arraystretch}{1.1}
		\begin{tabular}{@{}c <{\enspace}@{}lccccc@{}}	\toprule
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{5}{c}{\textbf{Dataset}}\\\cmidrule{3-7}
			& & {\textsc{Mcf-7}}         &  {\textsc{Molt-4}}      & {\textsc{Triangles}}           & {\textsc{github\_stargazers}}       & {\textsc{reddit\_threads}}      \\	\toprule
							\multirow{3}{*}{\rotatebox{90}{Kernel}}	& \text{1-WL}    & 
							   \scriptsize $\pm   \pm $       & \scriptsize $\pm   \pm $ & \scriptsize $\pm \pm$ &  \scriptsize $\pm \pm$ & \scriptsize $\pm \pm$  \\
		
			& \text{GR}            &   \scriptsize $\pm    \pm  $       &   \scriptsize $\pm \pm $ & \scriptsize $\pm \pm$ &  \scriptsize $\pm \pm$ & \scriptsize $\pm \pm$  \\  
			& \text{SP}             &    \scriptsize $\pm  \pm $       &  \scriptsize $\pm  \pm  $ & \scriptsize $\pm \pm$ &  \scriptsize $\pm \pm$ & \scriptsize $\pm \pm$  \\   
			\cmidrule{2-7}	
			\multirow{2}{*}{\rotatebox{90}{GNN}}	&  \text{GIN-E-$\varepsilon$}         &    \scriptsize $\pm  \pm $       & \scriptsize $\pm \pm $ & \scriptsize $\pm \pm$ &  \scriptsize $\pm \pm$ & \scriptsize $\pm \pm$ \\
			& \text{GraphSage}            &    \scriptsize $\pm \pm$       & \scriptsize $\pm \pm$ & \scriptsize $\pm \pm$ &  \scriptsize $\pm \pm$ & \scriptsize $\pm \pm $  \\
			\bottomrule
	\end{tabular}}
\end{table*}		




\begin{table}[t]\centering		
	\caption{Results for neural and kernel baselines.}
	\label{t2l}	
	\resizebox{0.55\columnwidth}{!}{ 	\renewcommand{\arraystretch}{1.1}
		\begin{tabular}{@{}c <{\enspace}@{}lcc@{}}	\toprule
			
			& \multirow{3}{*}{\vspace*{4pt}\textbf{Method}}&\multicolumn{2}{c}{\textbf{Dataset}}\\\cmidrule{3-4}
			&  & {\textsc{Zinc}}         &  {\textsc{Alchemy}}         \\	
			\toprule
			\multirow{3}{*}{\rotatebox{90}{Kernel}}	& \text{1-WL}             &      \scriptsize $$ &  \scriptsize $$  \\  
				& \text{GR}            &\scriptsize $ $ &  \scriptsize $ $  \\  
			& \text{SP}             &   \scriptsize $ $  &  \scriptsize $ $ \\   
			\cmidrule{2-4}
			\multirow{2}{*}{\rotatebox{90}{}} & \text{$1$-GNN-E}             &   \scriptsize $\pm $  & \scriptsize $\pm   $		\\	
			\bottomrule
	\end{tabular}}
\end{table}		



\section{Conclusion}

We gave an overview of the \textsc{Tud dataset}, and reported on the results of an experimental study comparing graph kernels and GNNs on a subset of the data. We believe that our dataset collection will spark further progress in the area of graph represention learning. We are looking forward to adding more dataset to our collection, and are excited about contributions from the community, and reseachers and practitionors from other areas.


\section{Acknowledgement}

We thank everybody who provided datasets for \textsc{Tud dataset}.

\bibliography{bibliography}
\bibliographystyle{icml2020}

\appendix

\onecolumn
\section{Evaluation Method Examples}
\lstinputlisting[language=Python]{example.py}
\cm{Add linear svm and GNN example}

\end{document}

