%%%%%%%% ICML 2020 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}



\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{booktabs}
\usepackage{url}
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{tikz}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{thmtools}		
\usepackage{mleftright}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{remark}[theorem]{Remark}

\usepackage{thm-restate}
\usepackage[mathic=true]{mathtools}
\usepackage{fixmath}
\usepackage{siunitx}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0.5\topsep}
\setlist[description]{noitemsep, topsep=0.5\topsep}
\setlist[itemize]{noitemsep, topsep=0.5\topsep}

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0.5\topsep}
\setlist[description]{noitemsep, topsep=0.5\topsep}
\setlist[itemize]{noitemsep, topsep=0.5\topsep}

\usepackage{setspace}
\usepackage[protrusion=true,expansion=false, activate={true,nocompatibility},final,kerning=true,spacing=true]{microtype}

\usepackage{ellipsis}
\usepackage{xspace}
\usepackage{hfoldsty}

\usepackage{ifthen}
\newcommand{\CC}[1][]{$\text{C\hspace{-.25ex}}^{_{_{_{++}}}}
	\ifthenelse{\equal{#1}{}}{}{\text{\hspace{-.625ex}#1}}$}

\newcommand{\win}[1]{$\hspace{-0.3mm}$\textbf{#1}}
\newcommand{\sd}[1]{\scriptsize{$\pm$#1}}


% Let cleveref and thmtools work together
\makeatletter
\def\thmt@refnamewithcomma #1#2#3,#4,#5\@nil{%
	\@xa\def\csname\thmt@envname #1utorefname\endcsname{#3}%
	\ifcsname #2refname\endcsname
	\csname #2refname\expandafter\endcsname\expandafter{\thmt@envname}{#3}{#4}%
	\fi
}
\makeatother
\usepackage[capitalise,noabbrev]{cleveref}   

\newcommand{\new}[1]{\emph{#1}}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\newcommand{\cA}{\ensuremath{{\mathcal A}}\xspace}
\newcommand{\cB}{\ensuremath{{\mathcal B}}\xspace}
\newcommand{\cC}{\ensuremath{{\mathcal C}}\xspace}
\newcommand{\cD}{\ensuremath{{\mathcal D}}\xspace}
\newcommand{\cF}{\ensuremath{{\mathcal F}}\xspace}
\newcommand{\cH}{\ensuremath{{\mathcal H}}\xspace}
\newcommand{\cN}{\ensuremath{{\mathcal N}}\xspace}
\newcommand{\cO}{\ensuremath{{\mathcal O}}\xspace}
\newcommand{\cP}{\ensuremath{{\mathcal P}}\xspace}
\newcommand{\cR}{\ensuremath{{\mathcal R}}\xspace}
\newcommand{\cS}{\ensuremath{{\mathcal S}}\xspace}
\newcommand{\cU}{\ensuremath{{\mathcal U}}\xspace}
\newcommand{\cV}{\ensuremath{{\mathcal V}}\xspace}
\newcommand{\cPn}{\ensuremath{{\mathcal P}_n}\xspace}

\newcommand{\fA}{\ensuremath{\mathfrak{A}}\xspace}
\newcommand{\fB}{\ensuremath{\mathfrak{B}}\xspace}
\newcommand{\fC}{\ensuremath{\mathfrak{C}}\xspace}

\newcommand{\fa}{\ensuremath{\mathfrak{a}}\xspace}
\newcommand{\fb}{\ensuremath{\mathfrak{b}}\xspace}
\newcommand{\fc}{\ensuremath{\mathfrak{c}}\xspace}
\newcommand{\fd}{\ensuremath{\mathfrak{d}}\xspace}

\newcommand{\bA}{\ensuremath{{\bf A}}\xspace}
\newcommand{\bB}{\ensuremath{{\bf B}}\xspace}
\newcommand{\bK}{\ensuremath{{\bf K}}\xspace}
\newcommand{\bE}{\ensuremath{{\bf E}}\xspace}
\newcommand{\bN}{\ensuremath{{\bf N}}\xspace}
\newcommand{\bG}{\ensuremath{{\bf G}}\xspace}

\newcommand{\ba}{\ensuremath{{\bf a}}\xspace}
\newcommand{\bb}{\ensuremath{{\bf b}}\xspace}
\newcommand{\bc}{\ensuremath{{\bf c}}\xspace}

\newcommand{\bbE}{\ensuremath{\mathbb{E}}}
\newcommand{\bbR}{\ensuremath{\mathbb{R}}}
\newcommand{\bbQ}{\ensuremath{\mathbb{Q}}}
\newcommand{\bbP}{\ensuremath{\mathbb{P}}}
\newcommand{\bbZ}{\ensuremath{\mathbb{Z}}}
\newcommand{\bbN}{\ensuremath{\mathbb{N}}}
\newcommand{\bbNn}{\ensuremath{\mathbb{N}_0}}

\newcommand{\bbRnp}{\ensuremath{\bbR_{\geq 0}}}
\newcommand{\bbQnp}{\ensuremath{\bbQ_{\geq}}}
\newcommand{\bbZnp}{\ensuremath{\bbZ_{\geq}}}

\newcommand{\bbRsp}{\ensuremath{\bbR_>}}
\newcommand{\bbQsp}{\ensuremath{\bbQ_>}}
\newcommand{\bbZsp}{\ensuremath{\bbZ_>}}

\newcommand{\cp}{\textsf{P}\xspace}
\newcommand{\cnp}{\textsf{NP}\xspace}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\GN}{\mathbb{G}_n}
\newcommand{\rb}{\right\}\xspace}
\newcommand{\lb}{\left\{\xspace}
\newcommand{\lbr}{\left(\xspace} 
\newcommand{\rbr}{\right)\xspace}
\newcommand{\ndelta}{\ensuremath{\overline{\delta}}}
\newcommand{\oms}{\{\!\!\{}
\newcommand{\cms}{\}\!\!\}}
\newcommand{\trans}{^T}
\renewcommand{\vec}[1]{\mathbf{#1}}

\usepackage{icml2020}





\icmltitlerunning{TUD Datasets}

\begin{document}

\twocolumn[
\icmltitle{TUD Datasets: A collection of benchmark datasets for learning with graphs}
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{C. Morris}{p}
\icmlauthor{N.\,M.\,Kriege}{vie}
\icmlauthor{K. Kersting}{darm}
\icmlauthor{P. Mutzw}{bonn}
\icmlauthor{N. Neumann}{wustl}
\end{icmlauthorlist}

\icmlaffiliation{p}{CERC in Data Science for Real-Time Decision-Making, Polytechnique Montr√©al}
\icmlaffiliation{vie}{University of Vienna}
\icmlaffiliation{darm}{Machine Learning Group, TU Darmstadt}
\icmlaffiliation{bonn}{Department of Computer Science, University of Bonn}
\icmlaffiliation{wustl}{Washington University in St. Louis}
\icmlcorrespondingauthor{Christopher Morris}{christopher.morris@tu-dortmund.de}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recently, there has been an increasing interest in learning with graph data, especially using graph neural networks.  However, the development of meaningful benchmark datasets and standardized evaluation procedures is lagging behind. That is, most paper papers evaluate their methods on small-scale datasets leading to high standard deviations and hard to interpret results, consequently hindering advancements in this area. To address this, we introduce the \textsc{Tud dataset} for graph classification and regression. The dataset consists of over 150 datasets from a wide range of applications and varying sizes. We provide Python-based data loaders, baseline implementations, and evaluation tools. Here, we give an overview of the datasets, evaluation tools, and provide baseline experiments. 
\end{abstract}

\section{Introduction}

Graph-structured data is ubiquitous across application domains ranging from chemo- and bioinformatics to image and social network analysis. To develop successful machine learning models in these domains, we need techniques that can exploit the rich information inherent in the graph structure, as well as the feature information contained within nodes and edges. In recent years, numerous approaches have been proposed for machine learning with graphs---most notably, approaches based on graph \new{kernels}~\cite{Kri+2019} or using \new{graph neural networks} (GNNs)~\cite{Gil+2017}. However, most papers, even recent ones, evaluate newly proposed architectures or methods on a fixed set of small-scale benchmark datasets leading to high standard deviations and hard to interpret results. 

Here, we give an overview of \new{Tud datasets}. The benchmark collection consists of over 150 datasets from a wide range of domains for supervised learning with graphs, i.e., classificiation and regression. All datasets are provided in a common dataset format at \url{graphlearning.io}, and easily be accessed from popular graph learning frameworks such as \emph{Pytorch Geometric}~\cite{Fey+2019} and \emph{DGL}~\cite{Wan+2019}.

\paragraph{Related work.}

There exists two approaches to supervised learing with graphs, graph kernels and graph neural networks (GNNs). Graph kernels have been studied extensively in the past 15 years, see ~\cite{Kri+2019} for a thorough overview. 
Important approaches include random-walk and shortest paths based kernels~\cite{Gaertner2003,Sugiyama2015,Bor+2005,Kri+2017b}, as well as the Weisfeiler-Lehman subtree kernel~\cite{She+2011,Mor+2017}. 
Further recent works focus on assignment-based approaches~\cite{Kri+2016,Nik+2017}, spectral approaches~\cite{Kon+2016}, and graph decomposition approaches~\cite{Nik+2018}.

Recently, GNNs~\cite{Gil+2017} emerged as a alternative to graph kernels. Notable instances of this model include~\cite{Duv+2015},~\cite{Li+2016},~\cite{Ham+2017} and the spectral approaches proposed in~\cite{Bru+2014,Def+2015,Kip+2017}---all of which descend from early work in~\cite{Kir+1995,Mer+2005,Sca+2009}. A unifying message passing architecture can be found in~\cite{Gil+2017}. Two recent surveys~\cite{wu2019comprehensive,zhou2018graph} provide a thorough overview of graph neural networks

The papers~\cite{Fey+2019,Err+2019,Dwi+2020} evalute GNNs using a unified evaluation procedure, however, both only use small scale datasets. Recently, \url{ogb.stanford.edu} launchend, however the provided datasets for graph classification focus on chemistry applications, and the number is quite limited at this point.



\paragraph{Contributions} We give an overview of \textsc{Tud dataset}, its unified evalution procudures, and baseline methods. Moreover, we report results on a experimental study comparing graph kernels and GNNs on a subset of the \textsc{Tud dataset}.

\section{Overview of the datasets}

The \textsc{Tud dataset} contain over 150 datasets provided at \url{graphlearning.io}. 

\section{Installation, usage, and evaluation tools}

\section{Experimental evaluation}

\subsection{Experimental protocol}

\section{Conclusion}




\bibliography{bibliography}
\bibliographystyle{icml2020}

\appendix


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2020. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
